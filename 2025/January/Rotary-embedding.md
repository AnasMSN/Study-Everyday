# Rotary Embedding
---------------------------------

## Positional Embedding

### What is positional embedding?
Positional embedding in context of transformer is one of the step which token will be added embed based on its position in the sentence. This is important because order of token will have an impact of the output of the result and the semantic meaning of sentence itself.

